# 手写数字识别  
## 1. 项目介绍  
手写数字识别是广为人知的深度学习和tensorflow的入门项目，今天我们就来用tensorflow搭建只有一个全连接层并且用softmax激活函数来识别手写数字的神经网络。  
## 2. 数据导入  
1. tensorflow自带手写数字识别的数据，我们可以通过`from tensorflow.examples.tutorials.mnist import input_data`来导入，然后写好数据存放的目录就可以读取数据了，第一次运行`input_data.read_data_sets`函数会用时长一点，因为程序会将数据存放在你写的目录下面，之后运行就会快很多，直接从保存数据的目录下面读取数据。  
2. 还有就是读取数据的时候，需要将其标签改为one_hot编码，因为神经网络的输出不是1、2、3等具体的数字，而是这10个数字也就是10个类，每个类的概率，概率最大的那个类代表的数字就是我们的预测结果。  
## 3. 设计计算图  
1. 神经网络的输入是一张图片的所有像素值，而我们导入的手写数字数据，图片数据是一个二维数组，一行代表一个数字图片，每一行有784个像素值，所以我们需要在计算图中设计一个输入张量，因为数据已经固定了，所以我们就用占位符来设置输入x。  
2. 有了输入层，还需要隐藏层也就是全连接层，因为需要10个输出，那我们设置隐藏层的神经元个数为10个，对应10个数字类别。那么我们需要的权重形状也就确定好了为[784,10]。权重是需要经过训练不断优化的，所以应该用变量来设置权重W。  
3. 目标值就是书写数字数据的真实标签值，我们导入的时候将其转换为one_hot编码，所以用形状为[-1, 10]的占位符来代表它。预测值就是x与W矩阵乘法的结果，这里就在计算图中写好矩阵乘法就好了，之后当图流动起来，会自然算好它。  
4. 神经网络训练的是权重，而更新权重是通过梯度下降算法，梯度下降需要用到的数据是预测值与真实值之间的交叉熵，也就是损失。我们使用的交叉熵是softmax交叉熵，也就是对y_pred值使用softmax函数进行激活后得到的值与真实值通过公式算出的损失。  
5. 有了损失的张量，我们就可以进行训练来优化损失了。优化损失需要用到优化器，这里我们选择随机梯度下降优化器——`GradientDescentOptimizer`，填好学习率，找到优化损失，直到损失最小。这时权重W也是最优的值，再跟X矩阵相乘得到的y_pred，就是我们最后确定的预测值了。  
6. 最后我们还需要评估我们的预测结果。比较y_pred每行最大值的索引是否与y_true每行最大值的索引相同，得到的是布尔值，为了计算准确率，我们需要将其转化为float数据类型，这样True等于1，False就等于0。将所有1求和然后除以行数我们就可以得到准确率了。  
## 4. 运行会话  
要让计算图流动起来，我们需要运行会话。  
1. 会话是一个上下文管理器，所以要使用with语法。  
2. 运行会话的第一步是初始化全局变量，不然无法使用变量。  
3. 然后就可以执行训练了，迭代次数有我们自己设置，在每次迭代的时候，真实的输入值通过`data.train.next_batch(MINIBATCH_SIZE)`这句代码传入进来，我们不可能每次都训练全部的数据，而是每次只训练一批数据，这批数据的大小也由自己设置。  
4. 通过`sess.run()`来让训练操作执行起来，里面还有一个参数feed_dict，这个参数需要传入字典，这就是为了赋予占位符数值的参数。  
5. 训练完之后就可以对测试集数据进行预测来得出准确率了，这里的占位符数据就要换成测试集的数据了。  
6. 最后运行我们可以得到91%左右的准确率。  
